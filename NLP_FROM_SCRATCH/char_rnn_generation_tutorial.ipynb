{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNLP From Scratch: Generating Names with a Character-Level RNN\n*************************************************************\n**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n\nThis is our second of three tutorials on \"NLP From Scratch\".\nIn the `first tutorial </intermediate/char_rnn_classification_tutorial>`\nwe used a RNN to classify names into their language of origin. This time\nwe'll turn around and generate names from languages.\n\n::\n\n    > python sample.py Russian RUS\n    Rovakov\n    Uantov\n    Shavakov\n\n    > python sample.py German GER\n    Gerren\n    Ereng\n    Rosher\n\n    > python sample.py Spanish SPA\n    Salla\n    Parer\n    Allan\n\n    > python sample.py Chinese CHI\n    Chan\n    Hang\n    Iun\n\nWe are still hand-crafting a small RNN with a few linear layers. The big\ndifference is instead of predicting a category after reading in all the\nletters of a name, we input a category and output one letter at a time.\nRecurrently predicting characters to form language (this could also be\ndone with words or other higher order constructs) is often referred to\nas a \"language model\".\n\n**Recommended Reading:**\n\nI assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:\n\n-  https://pytorch.org/ For installation instructions\n-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n\nIt would also be useful to know about RNNs and how they work:\n\n-  `The Unreasonable Effectiveness of Recurrent Neural\n   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n   shows a bunch of real life examples\n-  `Understanding LSTM\n   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n   is about LSTMs specifically but also informative about RNNs in\n   general\n\nI also suggest the previous tutorial, :doc:`/intermediate/char_rnn_classification_tutorial`\n\n\nPreparing the Data\n==================\n\n.. Note::\n   Download the data from\n   `here <https://download.pytorch.org/tutorial/data.zip>`_\n   and extract it to the current directory.\n\nSee the last tutorial for more detail of this process. In short, there\nare a bunch of plain text files ``data/names/[Language].txt`` with a\nname per line. We split lines into an array, convert Unicode to ASCII,\nand end up with a dictionary ``{language: [names ...]}``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "# categories: 18 ['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\nO'Neal\n"
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'Néàl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the Network\n====================\n\nThis network extends `the last tutorial's RNN <#Creating-the-Network>`__\nwith an extra argument for the category tensor, which is concatenated\nalong with the others. The category tensor is a one-hot vector just like\nthe letter input.\n\nWe will interpret the output as the probability of the next letter. When\nsampling, the most likely output letter is used as the next input\nletter.\n\nI added a second linear layer ``o2o`` (after combining hidden and\noutput) to give it more muscle to work with. There's also a dropout\nlayer, which `randomly zeros parts of its\ninput <https://arxiv.org/abs/1207.0580>`__ with a given probability\n(here 0.1) and is usually used to fuzz inputs to prevent overfitting.\nHere we're using it towards the end of the network to purposely add some\nchaos and increase sampling variety.\n\n.. figure:: https://i.imgur.com/jzVrf7f.png\n   :alt:\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n=========\nPreparing for Training\n----------------------\n\nFirst of all, helper functions to get random pairs of (category, line):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random line from that category\n",
        "def randomTrainigPair():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    return category, line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each timestep (that is, for each letter in a training word) the\ninputs of the network will be\n``(category, current letter, hidden state)`` and the outputs will be\n``(next letter, next hidden state)``. So for each training set, we'll\nneed the category, a set of input letters, and a set of output/target\nletters.\n\nSince we are predicting the next letter from the current letter for each\ntimestep, the letter pairs are groups of consecutive letters from the\nline - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"),\n(\"C\", \"D\"), (\"D\", \"EOS\").\n\n.. figure:: https://i.imgur.com/JH58tXY.png\n   :alt:\n\nThe category tensor is a `one-hot\ntensor <https://en.wikipedia.org/wiki/One-hot>`__ of size\n``<1 x n_categories>``. When training we feed it to the network at every\ntimestep - this is a design choice, it could have been included as part\nof initial hidden state or some other strategy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot vector for category\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1)  # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience during training we'll make a ``randomTrainingExample``\nfunction that fetches a random (category, line) pair and turns them into\nthe required (category, input, target) tensors.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainigPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the Network\n--------------------\n\nIn contrast to classification, where only the last output is used, we\nare making a prediction at every step, so we are calculating loss at\nevery step.\n\nThe magic of autograd allows you to simply sum these losses at each step\nand call backward at the end.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To keep track of how long training takes I am adding a\n``timeSince(timestamp)`` function which returns a human readable string:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport math\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training is business as usual - call train a bunch of times and wait a\nfew minutes, printing the current time and loss every ``print_every``\nexamples, and keeping store of an average loss per ``plot_every`` examples\nin ``all_losses`` for plotting later.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0m 16s (5000 5%) 2.7395\n0m 33s (10000 10%) 2.8086\n0m 50s (15000 15%) 2.4387\n1m 7s (20000 20%) 2.6012\n1m 24s (25000 25%) 2.0232\n1m 41s (30000 30%) 2.0558\n1m 57s (35000 35%) 2.0790\n2m 14s (40000 40%) 3.6042\n2m 31s (45000 45%) 1.0940\n2m 47s (50000 50%) 2.9506\n3m 3s (55000 55%) 2.3728\n3m 19s (60000 60%) 1.9716\n3m 35s (65000 65%) 2.1816\n3m 52s (70000 70%) 1.6329\n4m 8s (75000 75%) 2.5685\n4m 24s (80000 80%) 1.9223\n4m 40s (85000 85%) 2.2025\n4m 57s (90000 90%) 2.3552\n5m 13s (95000 95%) 2.8920\n5m 29s (100000 100%) 2.6595\n"
        }
      ],
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0  # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the Losses\n-------------------\n\nPlotting the historical loss from all\\_losses shows the network\nlearning:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[<matplotlib.lines.Line2D at 0x11ef43450>]"
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md7a3d555f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"89.920445\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(83.557945 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.157082\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(121.794582 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.39372\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(160.03122 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.630358\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(195.086608 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.866995\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(233.323245 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.103633\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(271.559883 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"319.340271\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(309.796521 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"357.576909\" xlink:href=\"#md7a3d555f4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(348.033159 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mce6ec4ce1c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"218.347287\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2.25 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 222.146506)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"189.184207\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.50 -->\n      <g transform=\"translate(7.2 192.983426)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"160.021127\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.75 -->\n      <g transform=\"translate(7.2 163.820346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"130.858047\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 3.00 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 134.657266)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"101.694967\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3.25 -->\n      <g transform=\"translate(7.2 105.494186)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"72.531887\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 3.50 -->\n      <g transform=\"translate(7.2 76.331105)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"43.368807\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 3.75 -->\n      <g transform=\"translate(7.2 47.168025)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mce6ec4ce1c\" y=\"14.205726\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 4.00 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(7.2 18.004945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p4ca80f8a78)\" d=\"M 51.683807 17.083636 \nL 53.213272 57.3287 \nL 54.742738 103.412342 \nL 56.272203 114.733144 \nL 57.801669 123.569906 \nL 59.331134 128.44579 \nL 60.8606 128.782312 \nL 62.390065 134.909479 \nL 63.919531 133.657679 \nL 65.448996 137.699167 \nL 66.978462 136.540699 \nL 68.507927 142.544348 \nL 70.037393 143.344635 \nL 71.566858 145.414446 \nL 73.096324 146.203864 \nL 74.625789 146.596514 \nL 76.155255 152.363229 \nL 77.68472 150.37042 \nL 79.214186 154.231107 \nL 80.743651 153.983917 \nL 82.273117 154.816239 \nL 83.802583 152.2772 \nL 85.332048 159.709531 \nL 86.861514 158.479699 \nL 88.390979 159.744989 \nL 89.920445 166.204813 \nL 91.44991 165.826277 \nL 92.979376 168.056868 \nL 94.508841 166.089183 \nL 96.038307 165.076853 \nL 97.567772 163.337927 \nL 99.097238 168.634137 \nL 100.626703 170.783382 \nL 102.156169 168.772642 \nL 103.685634 172.423891 \nL 105.2151 171.407663 \nL 106.744565 173.695219 \nL 108.274031 172.514217 \nL 109.803496 171.174879 \nL 111.332962 174.75791 \nL 112.862427 177.574528 \nL 114.391893 175.292704 \nL 115.921358 174.605361 \nL 117.450824 177.859709 \nL 118.980289 176.904655 \nL 120.509755 178.077415 \nL 122.03922 177.987846 \nL 123.568686 179.012067 \nL 125.098151 180.537861 \nL 126.627617 185.089889 \nL 128.157082 176.716591 \nL 129.686548 182.378965 \nL 131.216013 179.77387 \nL 132.745479 184.976909 \nL 134.274944 180.654286 \nL 135.80441 184.842752 \nL 137.333875 182.604885 \nL 138.863341 183.212083 \nL 140.392806 184.892655 \nL 141.922272 184.330455 \nL 143.451737 190.65623 \nL 144.981203 181.663688 \nL 146.510668 188.440416 \nL 148.040134 187.585587 \nL 149.569599 188.2275 \nL 151.099065 191.216897 \nL 152.62853 188.100232 \nL 154.157996 183.352375 \nL 155.687461 189.405784 \nL 157.216927 192.076882 \nL 158.746392 191.787046 \nL 160.275858 192.017414 \nL 161.805323 191.203717 \nL 163.334789 191.87843 \nL 164.864255 188.463601 \nL 166.39372 195.567733 \nL 167.923186 189.828436 \nL 169.452651 192.977478 \nL 170.982117 195.104401 \nL 172.511582 190.745973 \nL 174.041048 195.587704 \nL 175.570513 190.15086 \nL 177.099979 194.683685 \nL 178.629444 195.495015 \nL 180.15891 197.604811 \nL 181.688375 194.878377 \nL 183.217841 198.614192 \nL 184.747306 195.726499 \nL 186.276772 197.498056 \nL 187.806237 193.826728 \nL 189.335703 193.230939 \nL 190.865168 197.656144 \nL 192.394634 194.971175 \nL 193.924099 194.81896 \nL 195.453565 194.055585 \nL 196.98303 196.92753 \nL 198.512496 195.99507 \nL 200.041961 202.831715 \nL 201.571427 197.174915 \nL 203.100892 204.553359 \nL 204.630358 202.570231 \nL 206.159823 198.246717 \nL 207.689289 201.479089 \nL 209.218754 197.64795 \nL 210.74822 202.989254 \nL 212.277685 196.63207 \nL 213.807151 196.088496 \nL 215.336616 203.774673 \nL 216.866082 203.055339 \nL 218.395547 199.51451 \nL 219.925013 200.639055 \nL 221.454478 202.94787 \nL 222.983944 200.849924 \nL 224.513409 197.811259 \nL 226.042875 198.763036 \nL 227.57234 212.146804 \nL 229.101806 206.488918 \nL 230.631271 199.598886 \nL 232.160737 202.990617 \nL 233.690202 199.307095 \nL 235.219668 203.185068 \nL 236.749133 200.701483 \nL 238.278599 199.508829 \nL 239.808064 207.344423 \nL 241.33753 207.451418 \nL 242.866995 204.103966 \nL 244.396461 207.137831 \nL 245.925927 199.358265 \nL 247.455392 196.853902 \nL 248.984858 204.63931 \nL 250.514323 207.37368 \nL 252.043789 201.434065 \nL 253.573254 201.259777 \nL 255.10272 206.640199 \nL 256.632185 202.717127 \nL 258.161651 205.419303 \nL 259.691116 202.783047 \nL 261.220582 208.00121 \nL 262.750047 206.297758 \nL 264.279513 203.158994 \nL 265.808978 201.706237 \nL 267.338444 208.766239 \nL 268.867909 202.102207 \nL 270.397375 206.409359 \nL 271.92684 201.73261 \nL 273.456306 204.897588 \nL 274.985771 208.653274 \nL 276.515237 204.831431 \nL 278.044702 205.402338 \nL 279.574168 207.988169 \nL 281.103633 205.498039 \nL 282.633099 209.805577 \nL 284.162564 208.589418 \nL 285.69203 206.869049 \nL 287.221495 203.289196 \nL 288.750961 206.146808 \nL 290.280426 210.025576 \nL 291.809892 201.985128 \nL 293.339357 204.608695 \nL 294.868823 202.656166 \nL 296.398288 209.182578 \nL 297.927754 212.079608 \nL 299.457219 207.566499 \nL 300.986685 208.619582 \nL 302.51615 205.192796 \nL 304.045616 203.689488 \nL 305.575081 206.38174 \nL 307.104547 208.003253 \nL 308.634012 207.814069 \nL 310.163478 207.007835 \nL 313.222409 213.480566 \nL 314.751874 209.996091 \nL 316.28134 208.304848 \nL 317.810805 213.45569 \nL 319.340271 208.346817 \nL 320.869736 209.73278 \nL 322.399202 208.165868 \nL 323.928667 209.933656 \nL 325.458133 206.299661 \nL 326.987599 209.623545 \nL 330.04653 212.438675 \nL 331.575995 210.052964 \nL 333.105461 210.145415 \nL 334.634926 212.804355 \nL 336.164392 212.94847 \nL 337.693857 210.519847 \nL 339.223323 211.689791 \nL 340.752788 214.756364 \nL 342.282254 210.437342 \nL 343.811719 208.254822 \nL 345.341185 212.486737 \nL 346.87065 208.587019 \nL 348.400116 209.566824 \nL 349.929581 210.127558 \nL 351.459047 212.337406 \nL 352.988512 211.795851 \nL 354.517978 213.267387 \nL 356.047443 209.806379 \nL 356.047443 209.806379 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4ca80f8a78\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampling the Network\n====================\n\nTo sample we give the network a letter and ask what the next one is,\nfeed that in as the next letter, and repeat until the EOS token.\n\n-  Create tensors for input category, starting letter, and empty hidden\n   state\n-  Create a string ``output_name`` with the starting letter\n-  Up to a maximum output length,\n\n   -  Feed the current letter to the network\n   -  Get the next letter from highest output, and next hidden state\n   -  If the letter is EOS, stop here\n   -  If a regular letter, add to ``output_name`` and continue\n\n-  Return the final name\n\n.. Note::\n   Rather than having to give it a starting letter, another\n   strategy would have been to include a \"start of string\" token in\n   training and have the network choose its own starting letter.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Rovanikov\nUantonov\nShinovev\nGarter\nEerreng\nRoures\nSonger\nParera\nAllano\nChan\nHan\nIha\nAraka\nBamamo\nChamia\nJanaka\nPoura\nNoumara\n"
        }
      ],
      "source": [
        "max_length = 20\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        category_tensor = categoryTensor(category)\n",
        "        input = inputTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = inputTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(category, start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "samples('German', 'GER')\n",
        "\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "samples('Chinese', 'CHI')\n",
        "\n",
        "samples('Japanese')\n",
        "samples('Japanese', 'JPN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercises\n=========\n\n-  Try with a different dataset of category -> line, for example:\n\n   -  Fictional series -> Character name\n   -  Part of speech -> Word\n   -  Country -> City\n\n-  Use a \"start of sentence\" token so that sampling can be done without\n   choosing a start letter\n-  Get better results with a bigger and/or better shaped network\n\n   -  Try the nn.LSTM and nn.GRU layers\n   -  Combine multiple of these RNNs as a higher level network\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}